{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original annotation headers: ['Filename', 'Label', 'x1', 'y1', 'x2', 'y2', 'Pixel_Distance']\n",
      "Original row count (long format, one line segment per row): 1374\n",
      "Long format data detected, converting to wide format (one row per image with all 6 points)...\n",
      "Converted to wide format: 458 images, each with 12 coordinate columns (x1,y1,...,x6,y6)\n",
      "Converted columns: ['Filename', 'x1', 'y1', 'x2', 'y2', 'seg1_label', 'x3', 'y3', 'x4', 'y4', 'seg2_label', 'x5', 'y5', 'x6', 'y6', 'seg3_label']\n",
      "Identified image column: Filename\n",
      "Successfully identified 6 point pairs: [('x1', 'y1'), ('x2', 'y2'), ('x3', 'y3'), ('x4', 'y4'), ('x5', 'y5'), ('x6', 'y6')]\n",
      "Annotation columns for regression (should be 12 columns, 6 point pairs): ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'x5', 'y5', 'x6', 'y6']\n",
      "Number of point pairs: 6\n",
      "Found 458 valid image matches.\n",
      "Available annotated samples: 458\n",
      "Number of unlabeled images: 721\n",
      "Using device: cpu\n",
      "Initialization completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "PROJECT_DIR = Path(r\"c:\\\\Users\\\\Charlotte\\\\Desktop\\\\dissertation\\\\US_new\")\n",
    "IMAGES_DIR = PROJECT_DIR / \"High_quality_images\"\n",
    "ANNOTATION_FILE = PROJECT_DIR / \"annotation_points_summary2.xlsx\"\n",
    "OUTPUT_FILE = PROJECT_DIR / \"annotation_points_predicted.xlsx\"\n",
    "\n",
    "assert IMAGES_DIR.exists(), f\"Image directory not found: {IMAGES_DIR}\"\n",
    "assert ANNOTATION_FILE.exists(), f\"Annotation Excel not found: {ANNOTATION_FILE}\"\n",
    "\n",
    "df_raw = pd.read_excel(ANNOTATION_FILE)\n",
    "print(\"Original annotation headers:\", list(df_raw.columns))\n",
    "print(\"Original row count (long format, one line segment per row):\", len(df_raw))\n",
    "\n",
    "is_long_format = 'Label' in df_raw.columns and df_raw.groupby('Filename').size().max() > 1\n",
    "\n",
    "if is_long_format:\n",
    "    print(\"Long format data detected, converting to wide format (one row per image with all 6 points)...\")\n",
    "    \n",
    "    def get_segment_order(label_str):\n",
    "        label_lower = str(label_str).lower()\n",
    "        if 'medial' in label_lower:\n",
    "            return 1\n",
    "        elif 'femoral' in label_lower:\n",
    "            return 2\n",
    "        elif 'lateral' in label_lower:\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df_raw['__seg_order__'] = df_raw['Label'].map(get_segment_order)\n",
    "    df_raw = df_raw.sort_values(['Filename', '__seg_order__'])\n",
    "    \n",
    "    grouped = df_raw.groupby('Filename')\n",
    "    \n",
    "    records = []\n",
    "    for fn, group in grouped:\n",
    "        if len(group) != 3:\n",
    "            print(f\"Warning: {fn} has {len(group)} line segments (expected 3), skipping\")\n",
    "            continue\n",
    "        \n",
    "        row_dict = {'Filename': fn}\n",
    "        for seg_idx, (_, seg_row) in enumerate(group.iterrows(), 1):\n",
    "            row_dict[f'x{seg_idx*2-1}'] = seg_row['x1']\n",
    "            row_dict[f'y{seg_idx*2-1}'] = seg_row['y1']\n",
    "            row_dict[f'x{seg_idx*2}'] = seg_row['x2']\n",
    "            row_dict[f'y{seg_idx*2}'] = seg_row['y2']\n",
    "            if seg_idx <= 3:\n",
    "                row_dict[f'seg{seg_idx}_label'] = seg_row['Label']\n",
    "        \n",
    "        records.append(row_dict)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Converted to wide format: {len(df)} images, each with 12 coordinate columns (x1,y1,...,x6,y6)\")\n",
    "    print(\"Converted columns:\", list(df.columns))\n",
    "else:\n",
    "    df = df_raw\n",
    "    print(\"Data is already in wide format, no conversion needed\")\n",
    "\n",
    "possible_image_cols = [\n",
    "    'Filename', 'filename', 'image', 'img', 'image_path', 'path', 'Image', 'ImageName', 'FileName'\n",
    "]\n",
    "image_col = None\n",
    "for c in df.columns:\n",
    "    if c in possible_image_cols:\n",
    "        image_col = c\n",
    "        break\n",
    "if image_col is None:\n",
    "    for c in df.columns:\n",
    "        if df[c].astype(str).str.contains(r\"\\.(jpg|jpeg|png|bmp|webp)$\", case=False, na=False).any():\n",
    "            image_col = c\n",
    "            break\n",
    "assert image_col is not None, \"Cannot automatically identify image file column, please specify image_col manually\"\n",
    "print(\"Identified image column:\", image_col)\n",
    "\n",
    "x_cols = [f'x{i}' for i in range(1, 7) if f'x{i}' in df.columns]\n",
    "y_cols = [f'y{i}' for i in range(1, 7) if f'y{i}' in df.columns]\n",
    "\n",
    "pairs: List[Tuple[str, str]] = []\n",
    "if len(x_cols) == len(y_cols) and len(x_cols) == 6:\n",
    "    for i in range(6):\n",
    "        pairs.append((x_cols[i], y_cols[i]))\n",
    "    target_cols: List[str] = [p for xy in pairs for p in xy]\n",
    "    print(f\"Successfully identified 6 point pairs: {pairs}\")\n",
    "else:\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    x_like = [c for c in df.columns if re.search(r\"(^|[^a-zA-Z])(x|X)(\\d+)?$|(_x$)|(_X$)\", str(c))]\n",
    "    y_like = [c for c in df.columns if re.search(r\"(^|[^a-zA-Z])(y|Y)(\\d+)?$|(_y$)|(_Y$)\", str(c))]\n",
    "    \n",
    "    if x_like and y_like:\n",
    "        def idx_of(col: str) -> int:\n",
    "            m = re.search(r\"(\\d+)$\", col)\n",
    "            return int(m.group(1)) if m else -1\n",
    "        x_sorted = sorted(x_like, key=lambda c: (idx_of(str(c)), str(c)))\n",
    "        y_sorted = sorted(y_like, key=lambda c: (idx_of(str(c)), str(c)))\n",
    "        for x_c, y_c in zip(x_sorted, y_sorted):\n",
    "            pairs.append((x_c, y_c))\n",
    "        target_cols: List[str] = [p for xy in pairs for p in xy]\n",
    "    else:\n",
    "        exclude_like = {'id', 'index', 'idx'}\n",
    "        target_cols = [c for c in numeric_cols if str(c).lower() not in exclude_like]\n",
    "\n",
    "assert len(target_cols) > 0, \"No valid annotation numeric columns found, please check Excel column names\"\n",
    "print(\"Annotation columns for regression (should be 12 columns, 6 point pairs):\", target_cols)\n",
    "print(f\"Number of point pairs: {len(pairs)}\")\n",
    "\n",
    "def normalize_filename(name: str) -> str:\n",
    "    name = str(name)\n",
    "    name = Path(name).stem\n",
    "    name = name.replace(\"_\", \" \")\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    name = re.sub(r\"[\\s_]*\\(?(\\d+)\\)?$\", r\" (\\1)\", name)\n",
    "    return name.lower()\n",
    "\n",
    "image_index = {}\n",
    "for p in IMAGES_DIR.rglob(\"*\"):\n",
    "    if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}:\n",
    "        key = normalize_filename(p.stem)\n",
    "        image_index[key] = p\n",
    "\n",
    "def resolve_image_path(name: str) -> Path:\n",
    "    key = normalize_filename(name)\n",
    "    return image_index.get(key, None)\n",
    "\n",
    "df[\"__image_path__\"] = df[image_col].map(resolve_image_path)\n",
    "exists_mask = df[\"__image_path__\"].notnull() & df[\"__image_path__\"].map(lambda p: p.exists())\n",
    "print(f\"Found {exists_mask.sum()} valid image matches.\")\n",
    "\n",
    "if not exists_mask.all():\n",
    "    missing_count = (~exists_mask).sum()\n",
    "    print(f\"Warning: {missing_count} records in the annotation table cannot find corresponding images and will be ignored.\")\n",
    "\n",
    "df_labeled = df[exists_mask].copy()\n",
    "print(\"Available annotated samples:\", len(df_labeled))\n",
    "\n",
    "all_images = [\n",
    "    p for p in IMAGES_DIR.rglob(\"*\") \n",
    "    if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "]\n",
    "set_labeled = set(df_labeled[\"__image_path__\"].map(lambda p: p.resolve()))\n",
    "unlabeled_images = [p for p in all_images if p.resolve() not in set_labeled]\n",
    "print(\"Number of unlabeled images:\", len(unlabeled_images))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "backbone.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def image_to_embedding(img_path: Path) -> np.ndarray:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        feat = backbone(x)\n",
    "        feat = feat.view(feat.size(0), -1).cpu().numpy()[0]\n",
    "        return feat.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction failed: {img_path} -> {e}\")\n",
    "        return np.zeros((512,), dtype=np.float32)\n",
    "\n",
    "print(\"Initialization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|█████████████████████████████████████████████████████████| 458/458 [00:33<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled embeddings shape: (458, 512) Unlabeled embeddings shape: (721, 512)\n",
      "Sample path: c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\High_quality_images\\Abbey 001_v1_contralateral_base (1).jpg\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from typing import Tuple\n",
    "\n",
    "CACHE_DIR = PROJECT_DIR / \"emb_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "LABELED_CACHE = CACHE_DIR / \"labeled_embeddings.npz\"\n",
    "UNLABELED_CACHE = CACHE_DIR / \"unlabeled_embeddings.npz\"\n",
    "\n",
    "def get_image_size(p: Path) -> Tuple[int, int]:\n",
    "    try:\n",
    "        with Image.open(p) as im:\n",
    "            return im.size  # (width, height)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read image size: {p} -> {e}\")\n",
    "        return (1, 1)\n",
    "\n",
    "# Add dimensions to labeled samples\n",
    "sizes_labeled = df_labeled[\"__image_path__\"].map(get_image_size)\n",
    "df_labeled[\"__w__\"] = sizes_labeled.map(lambda s: s[0])\n",
    "df_labeled[\"__h__\"] = sizes_labeled.map(lambda s: s[1])\n",
    "\n",
    "# Dimensions of unlabeled samples\n",
    "sizes_unlabeled = [get_image_size(p) for p in unlabeled_images]\n",
    "\n",
    "def compute_embeddings(paths: List[Path]) -> np.ndarray:\n",
    "    feats = []\n",
    "    for p in tqdm(paths, desc=\"Extracting embeddings\"):\n",
    "        feats.append(image_to_embedding(p))\n",
    "    return np.stack(feats, axis=0) if len(feats) else np.zeros((0, 512), dtype=np.float32)\n",
    "\n",
    "# Cache or load labeled embeddings\n",
    "if LABELED_CACHE.exists():\n",
    "    z = np.load(LABELED_CACHE, allow_pickle=True)\n",
    "    emb_labeled = z[\"embeddings\"]\n",
    "    cached_paths = list(z[\"paths\"])  # as list of strings\n",
    "    cur_paths = [str(p) for p in df_labeled[\"__image_path__\"].tolist()]\n",
    "    # Recompute if cached paths don't match current paths or dimensions mismatch\n",
    "    if cached_paths != cur_paths or emb_labeled.shape[0] != len(cur_paths):\n",
    "        emb_labeled = compute_embeddings(df_labeled[\"__image_path__\"].tolist())\n",
    "        np.savez_compressed(LABELED_CACHE, embeddings=emb_labeled, paths=np.array(cur_paths, dtype=object))\n",
    "else:\n",
    "    emb_labeled = compute_embeddings(df_labeled[\"__image_path__\"].tolist())\n",
    "    np.savez_compressed(LABELED_CACHE, embeddings=emb_labeled, paths=np.array([str(p) for p in df_labeled[\"__image_path__\"].tolist()], dtype=object))\n",
    "\n",
    "# Cache or load unlabeled embeddings\n",
    "if UNLABELED_CACHE.exists():\n",
    "    z = np.load(UNLABELED_CACHE, allow_pickle=True)\n",
    "    emb_unlabeled = z[\"embeddings\"]\n",
    "    cached_paths_un = list(z[\"paths\"])  # as list of strings\n",
    "    cur_paths_un = [str(p) for p in unlabeled_images]\n",
    "    # Recompute if cached paths don't match current paths or dimensions mismatch\n",
    "    if cached_paths_un != cur_paths_un or emb_unlabeled.shape[0] != len(cur_paths_un):\n",
    "        emb_unlabeled = compute_embeddings(unlabeled_images)\n",
    "        np.savez_compressed(UNLABELED_CACHE, embeddings=emb_unlabeled, paths=np.array(cur_paths_un, dtype=object))\n",
    "else:\n",
    "    emb_unlabeled = compute_embeddings(unlabeled_images)\n",
    "    np.savez_compressed(UNLABELED_CACHE, embeddings=emb_unlabeled, paths=np.array([str(p) for p in unlabeled_images], dtype=object))\n",
    "\n",
    "print(\"Labeled embeddings shape:\", emb_labeled.shape, \"Unlabeled embeddings shape:\", emb_unlabeled.shape)\n",
    "print(\"Sample path:\", df_labeled[\"__image_path__\"].iloc[0] if len(df_labeled) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results saved to: c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_v1762770463.xlsx\n",
      "Completed: KNN-based coordinate prediction using visual similarity reference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "has_pairs = len(pairs) > 0\n",
    "\n",
    "def normalize_targets(df_like: pd.DataFrame, target_cols: List[str], pairs: List[Tuple[str, str]]):\n",
    "    Y = df_like[target_cols].to_numpy(dtype=float)\n",
    "    if not has_pairs:\n",
    "        return Y, None\n",
    "    Yn = Y.copy()\n",
    "    for i, (xc, yc) in enumerate(pairs):\n",
    "        x_idx = target_cols.index(xc)\n",
    "        y_idx = target_cols.index(yc)\n",
    "        w = df_like[\"__w__\"].to_numpy(dtype=float)\n",
    "        h = df_like[\"__h__\"].to_numpy(dtype=float)\n",
    "        Yn[:, x_idx] = Y[:, x_idx] / np.clip(w, 1.0, None)\n",
    "        Yn[:, y_idx] = Y[:, y_idx] / np.clip(h, 1.0, None)\n",
    "    return Yn, (\"paired\",)\n",
    "\n",
    "Yn_labeled, _ = normalize_targets(df_labeled, target_cols, pairs)\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, weights='distance', metric='cosine')\n",
    "knn.fit(emb_labeled, Yn_labeled)\n",
    "\n",
    "MAX_COSINE_DISTANCE = 0.6\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=5, metric='cosine').fit(emb_labeled)\n",
    "dists, _ = nbrs.kneighbors(emb_unlabeled, return_distance=True)\n",
    "median_dist = np.median(dists, axis=1) if dists.size else np.array([])\n",
    "\n",
    "Ypred_norm = knn.predict(emb_unlabeled) if emb_unlabeled.shape[0] > 0 else np.zeros((0, len(target_cols)))\n",
    "\n",
    "if has_pairs and len(unlabeled_images) > 0:\n",
    "    widths = np.array([w for (w, h) in sizes_unlabeled], dtype=float)\n",
    "    heights = np.array([h for (w, h) in sizes_unlabeled], dtype=float)\n",
    "    Ypred = Ypred_norm.copy()\n",
    "    for (xc, yc) in pairs:\n",
    "        x_idx = target_cols.index(xc)\n",
    "        y_idx = target_cols.index(yc)\n",
    "        Ypred[:, x_idx] = Ypred_norm[:, x_idx] * np.clip(widths, 1.0, None)\n",
    "        Ypred[:, y_idx] = Ypred_norm[:, y_idx] * np.clip(heights, 1.0, None)\n",
    "else:\n",
    "    Ypred = Ypred_norm\n",
    "\n",
    "if median_dist.size:\n",
    "    far_mask = median_dist > MAX_COSINE_DISTANCE\n",
    "    if far_mask.any():\n",
    "        Ypred[far_mask, :] = np.nan\n",
    "        print(f\"{int(far_mask.sum())} unlabeled samples are too dissimilar to labeled samples, set to NaN to avoid misleading predictions.\")\n",
    "\n",
    "unlabeled_names = [p.name for p in unlabeled_images]\n",
    "res_df = pd.DataFrame({\"filename\": unlabeled_names})\n",
    "for i, col in enumerate(target_cols):\n",
    "    res_df[col] = Ypred[:, i] if Ypred.shape[0] else []\n",
    "\n",
    "# Add three label columns (fixed labels since order is confirmed: seg1=medial, seg2=femoral, seg3=lateral)\n",
    "res_df[\"seg1_label\"] = \"Medial condyle\"\n",
    "res_df[\"seg2_label\"] = \"Femoral condyle\"\n",
    "res_df[\"seg3_label\"] = \"lateral_thickness\"\n",
    "\n",
    "out_path = OUTPUT_FILE\n",
    "if out_path.exists():\n",
    "    stem = out_path.stem\n",
    "    out_path = out_path.with_name(f\"{stem}_v{int(time())}.xlsx\")\n",
    "\n",
    "res_df.to_excel(out_path, index=False)\n",
    "print(\"Prediction results saved to:\", out_path)\n",
    "\n",
    "print(\"Completed: KNN-based coordinate prediction using visual similarity reference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line segment results saved to: c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_segments.xlsx\n"
     ]
    }
   ],
   "source": [
    "assert 'Ypred' in globals(), \"Please run the previous cell first to generate Ypred\"\n",
    "assert 'unlabeled_images' in globals(), \"Variable unlabeled_images does not exist\"\n",
    "\n",
    "num_pairs = len(pairs) if len(pairs) > 0 else (len(target_cols) // 2)\n",
    "if num_pairs != 6:\n",
    "    print(f\"Note: Currently detected {num_pairs} point pairs, which is inconsistent with the expected 3 line segments (=6 points). Will export as much as possible in the current order.\")\n",
    "\n",
    "# Construct line segment column names\n",
    "seg_cols = []\n",
    "for seg_idx in range(3):\n",
    "    seg_cols.extend([\n",
    "        f\"seg{seg_idx+1}_x1\", f\"seg{seg_idx+1}_y1\", f\"seg{seg_idx+1}_x2\", f\"seg{seg_idx+1}_y2\"\n",
    "    ])\n",
    "\n",
    "# Extract first 12 columns from Ypred (fill with NaN if insufficient, truncate if excess)\n",
    "P = np.full((Ypred.shape[0], 12), np.nan, dtype=float)\n",
    "use_cols = min(12, Ypred.shape[1])\n",
    "P[:, :use_cols] = Ypred[:, :use_cols]\n",
    "\n",
    "seg_df = pd.DataFrame({\"filename\": [p.name for p in unlabeled_images]})\n",
    "for i, col in enumerate(seg_cols):\n",
    "    seg_df[col] = P[:, i]\n",
    "\n",
    "# Add three label columns\n",
    "seg_df[\"seg1_label\"] = \"Medial condyle\"\n",
    "seg_df[\"seg2_label\"] = \"Femoral condyle\"\n",
    "seg_df[\"seg3_label\"] = \"lateral_thickness\"\n",
    "\n",
    "# Optional: Calculate length of each line segment (in pixels)\n",
    "for seg_idx in range(3):\n",
    "    x1 = seg_df[f\"seg{seg_idx+1}_x1\"].to_numpy(dtype=float)\n",
    "    y1 = seg_df[f\"seg{seg_idx+1}_y1\"].to_numpy(dtype=float)\n",
    "    x2 = seg_df[f\"seg{seg_idx+1}_x2\"].to_numpy(dtype=float)\n",
    "    y2 = seg_df[f\"seg{seg_idx+1}_y2\"].to_numpy(dtype=float)\n",
    "    seg_df[f\"seg{seg_idx+1}_length\"] = np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "\n",
    "out2 = OUTPUT_FILE.with_name(OUTPUT_FILE.stem + \"_segments.xlsx\")\n",
    "seg_df.to_excel(out2, index=False)\n",
    "print(\"Line segment results saved to:\", out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnet+KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted summary saved as c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 文件路径\n",
    "pred_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_segments.xlsx\"\n",
    "output_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_summary.xlsx\"\n",
    "\n",
    "# 转换参数\n",
    "pixel_to_mm = 0.003985\n",
    "linear_slope = 0.5752\n",
    "linear_intercept = 0.0743\n",
    "\n",
    "# 读取预测数据\n",
    "df = pd.read_excel(pred_file)\n",
    "df['seg1_cm_corrected'] = df['seg1_length'] * 0.003985 * linear_slope + linear_intercept\n",
    "df['seg2_cm_corrected'] = df['seg2_length'] * 0.003985 * linear_slope + linear_intercept\n",
    "df['seg3_cm_corrected'] = df['seg3_length'] * 0.003985 * linear_slope + linear_intercept\n",
    "\n",
    "# 先转换长度：像素 -> mm -> 线性校正\n",
    "for i in range(1,4):\n",
    "    df[f'seg{i}_mm_raw'] = df[f'seg{i}_length'] * pixel_to_mm\n",
    "    df[f'seg{i}_mm_corrected'] = df[f'seg{i}_mm_raw'] * linear_slope + linear_intercept\n",
    "\n",
    "# 定义函数提取 Patient / Stage / Location / Part\n",
    "def extract_info(filename, label):\n",
    "    fn = str(filename)\n",
    "    lb = str(label).lower()\n",
    "\n",
    "    # Patient: Abbey 024_v1 → Abbey 024\n",
    "    patient_match = re.search(r\"Abbey[_\\s]*\\d{1,3}\", fn, re.I)\n",
    "    patient = patient_match.group(0).replace(\"_\", \" \").strip() if patient_match else None\n",
    "\n",
    "    # Stage: v1/v2/v3/v4/v5 → base/3/4/5\n",
    "    stage = None\n",
    "    if \"v1\" in fn.lower():\n",
    "        stage = \"base\"\n",
    "    elif \"v3\" in fn.lower():\n",
    "        stage = \"3\"\n",
    "    elif \"v4\" in fn.lower():\n",
    "        stage = \"4\"\n",
    "    elif \"v5\" in fn.lower():\n",
    "        stage = \"5\"\n",
    "    else:\n",
    "        stage = \"base\"\n",
    "\n",
    "    # Location\n",
    "    location = \"contralateral\" if \"contralateral\" in fn.lower() else (\"treat\" if \"treat\" in fn.lower() else None)\n",
    "\n",
    "    # Part\n",
    "    if \"medial\" in lb:\n",
    "        part = \"medial\"\n",
    "    elif \"femoral\" in lb:\n",
    "        part = \"femoral\"\n",
    "    elif \"lateral\" in lb:\n",
    "        part = \"lateral\"\n",
    "    else:\n",
    "        part = None\n",
    "\n",
    "    return patient, stage, location, part\n",
    "\n",
    "# 提取信息\n",
    "records = []\n",
    "for idx, row in df.iterrows():\n",
    "    for i in range(1,4):\n",
    "        patient, stage, location, part = extract_info(row['filename'], row[f'seg{i}_label'])\n",
    "        value = row[f'seg{i}_mm_corrected']\n",
    "        records.append({\n",
    "            'Patient': patient,\n",
    "            'Stage': stage,\n",
    "            'Location': location,\n",
    "            'Part': part,\n",
    "            'Predicted_mm': value\n",
    "        })\n",
    "\n",
    "pred_df_long = pd.DataFrame(records)\n",
    "\n",
    "# 分组平均（同患者/阶段/部位可能有多张图）\n",
    "grouped = pred_df_long.groupby(['Patient','Stage','Location','Part'])['Predicted_mm'].mean().reset_index()\n",
    "\n",
    "# 透视表\n",
    "pivot = grouped.pivot_table(index='Patient', columns=['Part','Location','Stage'], values='Predicted_mm')\n",
    "\n",
    "# 重命名列\n",
    "pivot.columns = [f\"US_{p}_{l}_{s}\" for (p,l,s) in pivot.columns]\n",
    "pivot.reset_index(inplace=True)\n",
    "\n",
    "# 固定列顺序（保持和你之前标注表一样）\n",
    "cols = [\"Patient\",\n",
    "    \"US_medial_treat_base\",\"US_femoral_treat_base\",\"US_lateral_treat_base\",\n",
    "    \"US_medial_contralateral_base\",\"US_femoral_contralateral_base\",\"US_lateral_contralateral_base\",\n",
    "    \"US_medial_treat_3\",\"US_femoral_treat_3\",\"US_lateral_treat_3\",\n",
    "    \"US_medial_contralateral_3\",\"US_femoral_contralateral_3\",\"US_lateral_contralateral_3\",\n",
    "    \"US_medial_treat_4\",\"US_femoral_treat_4\",\"US_lateral_treat_4\",\n",
    "    \"US_medial_contralateral_4\",\"US_femoral_contralateral_4\",\"US_lateral_contralateral_4\",\n",
    "    \"US_medial_treat_5\",\"US_femoral_treat_5\",\"US_lateral_treat_5\",\n",
    "    \"US_medial_contralateral_5\",\"US_femoral_contralateral_5\",\"US_lateral_contralateral_5\"\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    if c not in pivot.columns:\n",
    "        pivot[c] = None\n",
    "pivot = pivot[cols]\n",
    "\n",
    "# 保存结果\n",
    "pivot.to_excel(output_file, index=False)\n",
    "print(f\"Predicted summary saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book-format summary saved as annotation_points_summary_averaged_formatted.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_excel(\"annotation_points_summary.xlsx\")\n",
    "\n",
    "def extract_info(filename, label):\n",
    "    fn = str(filename)\n",
    "    lb = str(label).lower()\n",
    "\n",
    "    # Patient: Abbey_001 → Abbey 001\n",
    "    patient_match = re.search(r\"Abbey[_\\s]*\\d{1,3}\", fn, re.I)\n",
    "    patient = patient_match.group(0).replace(\"_\", \" \").strip() if patient_match else None\n",
    "\n",
    "    # Stage: 匹配 base 或 v3/v4/v5\n",
    "    stage_match = re.search(r\"(base|v\\d+)\", fn, re.I)\n",
    "    # Stage: 修正版\n",
    "    stage = None\n",
    "    if \"v1\" in fn.lower():\n",
    "        stage = \"base\"\n",
    "    elif \"v3\" in fn.lower():\n",
    "        stage = \"3\"\n",
    "    elif \"v4\" in fn.lower():\n",
    "        stage = \"4\"\n",
    "    elif \"v5\" in fn.lower():\n",
    "        stage = \"5\"\n",
    "    else:\n",
    "        # 兜底匹配 base\n",
    "        if \"base\" in fn.lower():\n",
    "            stage = \"base\"\n",
    "\n",
    "\n",
    "    # Location\n",
    "    location = \"contralateral\" if \"contralateral\" in fn.lower() else (\"treat\" if \"treat\" in fn.lower() else None)\n",
    "\n",
    "    # Part from Label\n",
    "    if \"medial\" in lb:\n",
    "        part = \"medial\"\n",
    "    elif \"femoral\" in lb:\n",
    "        part = \"femoral\"\n",
    "    elif \"lateral\" in lb:\n",
    "        part = \"lateral\"\n",
    "    else:\n",
    "        part = None\n",
    "\n",
    "    return patient, stage, location, part\n",
    "\n",
    "# 提取\n",
    "df[[\"Patient\", \"Stage\", \"Location\", \"Part\"]] = df.apply(lambda x: pd.Series(extract_info(x[\"Filename\"], x[\"Label\"])), axis=1)\n",
    "\n",
    "# 分组平均\n",
    "grouped = df.groupby([\"Patient\", \"Stage\", \"Location\", \"Part\"])[\"Pixel_Distance\"].mean().reset_index()\n",
    "\n",
    "# 透视\n",
    "pivot = grouped.pivot_table(index=\"Patient\", columns=[\"Part\",\"Location\",\"Stage\"], values=\"Pixel_Distance\")\n",
    "pivot.columns = [f\"US_{p}_{l}_{s}\" for (p,l,s) in pivot.columns]\n",
    "pivot.reset_index(inplace=True)\n",
    "\n",
    "# 固定列顺序\n",
    "cols = [\"Patient\",\n",
    "    \"US_medial_treat_base\",\"US_femoral_treat_base\",\"US_lateral_treat_base\",\n",
    "    \"US_medial_contralateral_base\",\"US_femoral_contralateral_base\",\"US_lateral_contralateral_base\",\n",
    "    \"US_medial_treat_3\",\"US_femoral_treat_3\",\"US_lateral_treat_3\",\n",
    "    \"US_medial_contralateral_3\",\"US_femoral_contralateral_3\",\"US_lateral_contralateral_3\",\n",
    "    \"US_medial_treat_4\",\"US_femoral_treat_4\",\"US_lateral_treat_4\",\n",
    "    \"US_medial_contralateral_4\",\"US_femoral_contralateral_4\",\"US_lateral_contralateral_4\",\n",
    "    \"US_medial_treat_5\",\"US_femoral_treat_5\",\"US_lateral_treat_5\",\n",
    "    \"US_medial_contralateral_5\",\"US_femoral_contralateral_5\",\"US_lateral_contralateral_5\"\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    if c not in pivot.columns:\n",
    "        pivot[c] = None\n",
    "pivot = pivot[cols]\n",
    "\n",
    "pivot.to_excel(\"annotation_points_summary_averaged_formatted.xlsx\", index=False)\n",
    "print(\"Book-format summary saved as annotation_points_summary_averaged_formatted.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall average:\n",
      "MAE_cm       0.047041\n",
      "RMSE_cm      0.059311\n",
      "R2          -0.698896\n",
      "Pearson_r    0.085707\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 文件路径\n",
    "pred_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_summary.xlsx\"\n",
    "manual_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\Book2.xlsx\"\n",
    "output_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\prediction_vs_manual_metrics_filtered.xlsx\"\n",
    "\n",
    "# 读取数据\n",
    "pred_df = pd.read_excel(pred_file)\n",
    "manual_df = pd.read_excel(manual_file)\n",
    "\n",
    "# 只保留 Book2 中存在的患者\n",
    "common_patients = manual_df['Patient'].unique()\n",
    "pred_df = pred_df[pred_df['Patient'].isin(common_patients)].copy()\n",
    "\n",
    "# 找到 US_* 列（预测表和手工表都可能存在空列）\n",
    "pred_cols = [c for c in pred_df.columns if c.startswith(\"US_\")]\n",
    "manual_cols = [c for c in manual_df.columns if c.startswith(\"US_\")]\n",
    "\n",
    "# 取两者的交集列\n",
    "common_cols = list(set(pred_cols).intersection(set(manual_cols)))\n",
    "\n",
    "if len(common_cols) == 0:\n",
    "    raise ValueError(\"No matching US_* columns found between predicted and manual data!\")\n",
    "\n",
    "# 合并预测和手工表\n",
    "merged_df = pd.merge(pred_df[['Patient'] + common_cols], manual_df[['Patient'] + common_cols], \n",
    "                     on='Patient', how='inner', suffixes=('_pred', '_manual'))\n",
    "\n",
    "# 创建结果列表\n",
    "results = []\n",
    "\n",
    "for col in common_cols:\n",
    "    pred_col = col + '_pred'\n",
    "    manual_col = col + '_manual'\n",
    "    \n",
    "    # 过滤掉缺失值\n",
    "    mask = (~merged_df[pred_col].isna()) & (~merged_df[manual_col].isna())\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    y_pred = merged_df.loc[mask, pred_col]\n",
    "    y_true = merged_df.loc[mask, manual_col]\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    r, _ = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'US_part': col,\n",
    "        'MAE_cm': mae,\n",
    "        'RMSE_cm': rmse,\n",
    "        'R2': r2,\n",
    "        'Pearson_r': r\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 文件路径\n",
    "pred_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_summary.xlsx\"\n",
    "manual_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\Book2.xlsx\"\n",
    "output_file = r\"c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\prediction_vs_manual_metrics_filtered.xlsx\"\n",
    "\n",
    "# 读取数据\n",
    "pred_df = pd.read_excel(pred_file)\n",
    "manual_df = pd.read_excel(manual_file)\n",
    "\n",
    "# 只保留 Book2 中存在的患者\n",
    "common_patients = manual_df['Patient'].unique()\n",
    "pred_df = pred_df[pred_df['Patient'].isin(common_patients)].copy()\n",
    "\n",
    "# 找到 US_* 列（预测表和手工表都可能存在空列）\n",
    "pred_cols = [c for c in pred_df.columns if c.startswith(\"US_\")]\n",
    "manual_cols = [c for c in manual_df.columns if c.startswith(\"US_\")]\n",
    "\n",
    "# 取两者的交集列\n",
    "common_cols = list(set(pred_cols).intersection(set(manual_cols)))\n",
    "\n",
    "if len(common_cols) == 0:\n",
    "    raise ValueError(\"No matching US_* columns found between predicted and manual data!\")\n",
    "\n",
    "# 合并预测和手工表\n",
    "merged_df = pd.merge(pred_df[['Patient'] + common_cols], manual_df[['Patient'] + common_cols], \n",
    "                     on='Patient', how='inner', suffixes=('_pred', '_manual'))\n",
    "\n",
    "# 创建结果列表\n",
    "results = []\n",
    "\n",
    "for col in common_cols:\n",
    "    pred_col = col + '_pred'\n",
    "    manual_col = col + '_manual'\n",
    "    \n",
    "    # 过滤掉缺失值\n",
    "    mask = (~merged_df[pred_col].isna()) & (~merged_df[manual_col].isna())\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    y_pred = merged_df.loc[mask, pred_col]\n",
    "    y_true = merged_df.loc[mask, manual_col]\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    r, _ = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'US_part': col,\n",
    "        'MAE_cm': mae,\n",
    "        'RMSE_cm': rmse,\n",
    "        'R2': r2,\n",
    "        'Pearson_r': r\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "#metrics_df.to_excel(output_file, index=False)\n",
    "#print(f\"Accuracy metrics saved to {output_file}\")\n",
    "\n",
    "# 输出整体平均指标\n",
    "if len(metrics_df) > 0:\n",
    "    print(\"Overall average:\")\n",
    "    print(metrics_df[['MAE_cm','RMSE_cm','R2','Pearson_r']].mean())\n",
    "else:\n",
    "    print(\"No metrics computed, check if predicted summary has any data for these patients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型优化方案\n",
    "\n",
    "当前KNN回归模型可能无法很好地捕捉坐标之间的复杂关系，导致预测的三个点坐标差距不明显。以下是几种优化方案：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练随机森林回归模型...\n",
      "随机森林预测结果已保存: c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_rf.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 方案1：使用随机森林回归（更强大的非线性模型）\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"训练随机森林回归模型...\")\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=2,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf.fit(emb_labeled, Yn_labeled)\n",
    "\n",
    "Ypred_rf_norm = rf.predict(emb_unlabeled) if emb_unlabeled.shape[0] > 0 else np.zeros((0, len(target_cols)))\n",
    "\n",
    "# 反归一化\n",
    "if has_pairs and len(unlabeled_images) > 0:\n",
    "    Ypred_rf = Ypred_rf_norm.copy()\n",
    "    for (xc, yc) in pairs:\n",
    "        x_idx = target_cols.index(xc)\n",
    "        y_idx = target_cols.index(yc)\n",
    "        Ypred_rf[:, x_idx] = Ypred_rf_norm[:, x_idx] * np.clip(widths, 1.0, None)\n",
    "        Ypred_rf[:, y_idx] = Ypred_rf_norm[:, y_idx] * np.clip(heights, 1.0, None)\n",
    "else:\n",
    "    Ypred_rf = Ypred_rf_norm\n",
    "\n",
    "# 导出随机森林预测结果\n",
    "res_df_rf = pd.DataFrame({\"filename\": unlabeled_names})\n",
    "for i, col in enumerate(target_cols):\n",
    "    res_df_rf[col] = Ypred_rf[:, i] if Ypred_rf.shape[0] else []\n",
    "\n",
    "res_df_rf[\"seg1_label\"] = \"Medial condyle\"\n",
    "res_df_rf[\"seg2_label\"] = \"Femoral condyle\"\n",
    "res_df_rf[\"seg3_label\"] = \"lateral_thickness\"\n",
    "\n",
    "out_rf = OUTPUT_FILE.with_name(OUTPUT_FILE.stem + \"_rf.xlsx\")\n",
    "res_df_rf.to_excel(out_rf, index=False)\n",
    "print(\"随机森林预测结果已保存:\", out_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost未安装，跳过。安装命令: pip install xgboost\n"
     ]
    }
   ],
   "source": [
    "# 方案2：使用XGBoost回归（梯度提升，通常效果更好）\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    print(\"训练XGBoost回归模型...\")\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    xgb_model.fit(emb_labeled, Yn_labeled)\n",
    "    \n",
    "    Ypred_xgb_norm = xgb_model.predict(emb_unlabeled) if emb_unlabeled.shape[0] > 0 else np.zeros((0, len(target_cols)))\n",
    "    \n",
    "    # 反归一化\n",
    "    if has_pairs and len(unlabeled_images) > 0:\n",
    "        Ypred_xgb = Ypred_xgb_norm.copy()\n",
    "        for (xc, yc) in pairs:\n",
    "            x_idx = target_cols.index(xc)\n",
    "            y_idx = target_cols.index(yc)\n",
    "            Ypred_xgb[:, x_idx] = Ypred_xgb_norm[:, x_idx] * np.clip(widths, 1.0, None)\n",
    "            Ypred_xgb[:, y_idx] = Ypred_xgb_norm[:, y_idx] * np.clip(heights, 1.0, None)\n",
    "    else:\n",
    "        Ypred_xgb = Ypred_xgb_norm\n",
    "    \n",
    "    # 导出XGBoost预测结果\n",
    "    res_df_xgb = pd.DataFrame({\"filename\": unlabeled_names})\n",
    "    for i, col in enumerate(target_cols):\n",
    "        res_df_xgb[col] = Ypred_xgb[:, i] if Ypred_xgb.shape[0] else []\n",
    "    \n",
    "    res_df_xgb[\"seg1_label\"] = \"Medial condyle\"\n",
    "    res_df_xgb[\"seg2_label\"] = \"Femoral condyle\"\n",
    "    res_df_xgb[\"seg3_label\"] = \"lateral_thickness\"\n",
    "    \n",
    "    out_xgb = OUTPUT_FILE.with_name(OUTPUT_FILE.stem + \"_xgb.xlsx\")\n",
    "    res_df_xgb.to_excel(out_xgb, index=False)\n",
    "    print(\"XGBoost预测结果已保存:\", out_xgb)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"XGBoost未安装，跳过。安装命令: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用集成方法（KNN + 随机森林 + XGBoost）...\n",
      "集成预测结果已保存: c:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_predicted_ensemble.xlsx\n",
      "使用了 2 个模型的加权平均，权重: [0.4 0.6]\n"
     ]
    }
   ],
   "source": [
    "# 方案3：集成方法 - 组合多个模型的预测结果（加权平均）\n",
    "print(\"使用集成方法（KNN + 随机森林 + XGBoost）...\")\n",
    "\n",
    "predictions_norm = [Ypred_norm]  # KNN\n",
    "if 'Ypred_rf_norm' in globals():\n",
    "    predictions_norm.append(Ypred_rf_norm)\n",
    "if 'Ypred_xgb_norm' in globals():\n",
    "    predictions_norm.append(Ypred_xgb_norm)\n",
    "\n",
    "# 加权平均（KNN权重较低，RF和XGBoost权重较高）\n",
    "weights = [0.3, 0.35, 0.35] if len(predictions_norm) == 3 else ([0.4, 0.6] if len(predictions_norm) == 2 else [1.0])\n",
    "if len(weights) > len(predictions_norm):\n",
    "    weights = weights[:len(predictions_norm)]\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "Ypred_ensemble_norm = np.average(predictions_norm, axis=0, weights=weights)\n",
    "\n",
    "# 反归一化\n",
    "if has_pairs and len(unlabeled_images) > 0:\n",
    "    Ypred_ensemble = Ypred_ensemble_norm.copy()\n",
    "    for (xc, yc) in pairs:\n",
    "        x_idx = target_cols.index(xc)\n",
    "        y_idx = target_cols.index(yc)\n",
    "        Ypred_ensemble[:, x_idx] = Ypred_ensemble_norm[:, x_idx] * np.clip(widths, 1.0, None)\n",
    "        Ypred_ensemble[:, y_idx] = Ypred_ensemble_norm[:, y_idx] * np.clip(heights, 1.0, None)\n",
    "else:\n",
    "    Ypred_ensemble = Ypred_ensemble_norm\n",
    "\n",
    "# 导出集成预测结果\n",
    "res_df_ensemble = pd.DataFrame({\"filename\": unlabeled_names})\n",
    "for i, col in enumerate(target_cols):\n",
    "    res_df_ensemble[col] = Ypred_ensemble[:, i] if Ypred_ensemble.shape[0] else []\n",
    "\n",
    "res_df_ensemble[\"seg1_label\"] = \"Medial condyle\"\n",
    "res_df_ensemble[\"seg2_label\"] = \"Femoral condyle\"\n",
    "res_df_ensemble[\"seg3_label\"] = \"lateral_thickness\"\n",
    "\n",
    "out_ensemble = OUTPUT_FILE.with_name(OUTPUT_FILE.stem + \"_ensemble.xlsx\")\n",
    "res_df_ensemble.to_excel(out_ensemble, index=False)\n",
    "print(\"集成预测结果已保存:\", out_ensemble)\n",
    "print(f\"使用了 {len(predictions_norm)} 个模型的加权平均，权重: {weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他优化建议\n",
    "\n",
    "1. **交叉验证调参**：使用交叉验证找到最优的模型超参数\n",
    "2. **数据增强**：对训练图像进行旋转、翻转等增强，增加数据多样性\n",
    "3. **几何约束后处理**：根据解剖学知识，对预测结果进行合理性检查\n",
    "4. **半监督学习**：使用预测结果作为伪标签，迭代改进模型\n",
    "5. **特征工程**：添加图像统计特征（如直方图、纹理特征）作为额外输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方案4：使用ResNet50提取更丰富的特征 + 随机森林/XGBoost\n",
    "# ResNet50提供2048维特征（vs ResNet18的512维），可能捕获更多细节\n",
    "\n",
    "print(\"使用ResNet50提取更丰富的特征...\")\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "backbone50 = nn.Sequential(*list(resnet50.children())[:-1]).to(device)\n",
    "backbone50.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def image_to_embedding_resnet50(img_path: Path) -> np.ndarray:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        feat = backbone50(x)  # [1, 2048, 1, 1]\n",
    "        feat = feat.view(feat.size(0), -1).cpu().numpy()[0]\n",
    "        return feat.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"提取特征失败: {img_path} -> {e}\")\n",
    "        return np.zeros((2048,), dtype=np.float32)\n",
    "\n",
    "# 提取ResNet50特征\n",
    "print(\"提取已标注图像的ResNet50特征...\")\n",
    "emb_labeled_r50 = np.stack([image_to_embedding_resnet50(p) for p in tqdm(df_labeled[\"__image_path__\"].tolist(), desc=\"ResNet50\")], axis=0)\n",
    "\n",
    "# 使用随机森林训练\n",
    "rf_r50 = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_r50.fit(emb_labeled_r50, Yn_labeled)\n",
    "\n",
    "# 提取未标注图像特征并预测\n",
    "print(\"提取未标注图像的ResNet50特征...\")\n",
    "emb_unlabeled_r50 = np.stack([image_to_embedding_resnet50(p) for p in tqdm(unlabeled_images, desc=\"ResNet50\")], axis=0)\n",
    "Ypred_rf_r50_norm = rf_r50.predict(emb_unlabeled_r50)\n",
    "\n",
    "# 反归一化\n",
    "if has_pairs and len(unlabeled_images) > 0:\n",
    "    Ypred_rf_r50 = Ypred_rf_r50_norm.copy()\n",
    "    for (xc, yc) in pairs:\n",
    "        x_idx = target_cols.index(xc)\n",
    "        y_idx = target_cols.index(yc)\n",
    "        Ypred_rf_r50[:, x_idx] = Ypred_rf_r50_norm[:, x_idx] * np.clip(widths, 1.0, None)\n",
    "        Ypred_rf_r50[:, y_idx] = Ypred_rf_r50_norm[:, y_idx] * np.clip(heights, 1.0, None)\n",
    "else:\n",
    "    Ypred_rf_r50 = Ypred_rf_r50_norm\n",
    "\n",
    "# 导出\n",
    "res_df_rf_r50 = pd.DataFrame({\"filename\": unlabeled_names})\n",
    "for i, col in enumerate(target_cols):\n",
    "    res_df_rf_r50[col] = Ypred_rf_r50[:, i] if Ypred_rf_r50.shape[0] else []\n",
    "\n",
    "res_df_rf_r50[\"seg1_label\"] = \"Medial condyle\"\n",
    "res_df_rf_r50[\"seg2_label\"] = \"Femoral condyle\"\n",
    "res_df_rf_r50[\"seg3_label\"] = \"lateral_thickness\"\n",
    "\n",
    "out_rf_r50 = OUTPUT_FILE.with_name(OUTPUT_FILE.stem + \"_rf_resnet50.xlsx\")\n",
    "res_df_rf_r50.to_excel(out_rf_r50, index=False)\n",
    "print(\"随机森林+ResNet50预测结果已保存:\", out_rf_r50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方案5：几何约束后处理 - 根据解剖学知识调整预测结果\n",
    "# 如果三个点的坐标差距不明显，可以通过分析已标注数据的统计特征来调整\n",
    "\n",
    "print(\"应用几何约束后处理...\")\n",
    "\n",
    "# 计算已标注数据中三个点之间的相对距离统计\n",
    "if len(df_labeled) > 0:\n",
    "    # 计算每条线段的长度\n",
    "    seg_lengths_labeled = []\n",
    "    for seg_idx in range(3):\n",
    "        x1_col = f'x{seg_idx*2+1}'\n",
    "        y1_col = f'y{seg_idx*2+1}'\n",
    "        x2_col = f'x{seg_idx*2+2}'\n",
    "        y2_col = f'y{seg_idx*2+2}'\n",
    "        if all(c in df_labeled.columns for c in [x1_col, y1_col, x2_col, y2_col]):\n",
    "            lengths = np.sqrt(\n",
    "                (df_labeled[x2_col] - df_labeled[x1_col])**2 + \n",
    "                (df_labeled[y2_col] - df_labeled[y1_col])**2\n",
    "            )\n",
    "            seg_lengths_labeled.append(lengths)\n",
    "    \n",
    "    if len(seg_lengths_labeled) == 3:\n",
    "        # 计算每条线段长度的均值和标准差\n",
    "        seg_means = [np.mean(l) for l in seg_lengths_labeled]\n",
    "        seg_stds = [np.std(l) for l in seg_lengths_labeled]\n",
    "        \n",
    "        print(f\"已标注数据线段长度统计:\")\n",
    "        print(f\"  seg1 (Medial): 均值={seg_means[0]:.2f}, 标准差={seg_stds[0]:.2f}\")\n",
    "        print(f\"  seg2 (Femoral): 均值={seg_means[1]:.2f}, 标准差={seg_stds[1]:.2f}\")\n",
    "        print(f\"  seg3 (Lateral): 均值={seg_means[2]:.2f}, 标准差={seg_stds[2]:.2f}\")\n",
    "        \n",
    "        # 对预测结果进行后处理：如果预测的线段长度与统计值差异过大，进行缩放调整\n",
    "        Ypred_adjusted = Ypred_ensemble.copy() if 'Ypred_ensemble' in globals() else Ypred.copy()\n",
    "        \n",
    "        for i in range(len(unlabeled_images)):\n",
    "            for seg_idx in range(3):\n",
    "                x1_idx = seg_idx * 2\n",
    "                y1_idx = seg_idx * 2 + 1\n",
    "                x2_idx = seg_idx * 2 + 2\n",
    "                y2_idx = seg_idx * 2 + 3\n",
    "                \n",
    "                if all(idx < Ypred_adjusted.shape[1] for idx in [x1_idx, y1_idx, x2_idx, y2_idx]):\n",
    "                    pred_length = np.sqrt(\n",
    "                        (Ypred_adjusted[i, x2_idx] - Ypred_adjusted[i, x1_idx])**2 +\n",
    "                        (Ypred_adjusted[i, y2_idx] - Ypred_adjusted[i, y1_idx])**2\n",
    "                    )\n",
    "                    \n",
    "                    # 如果预测长度与均值差异超过2个标准差，进行缩放\n",
    "                    if seg_means[seg_idx] > 0 and abs(pred_length - seg_means[seg_idx]) > 2 * seg_stds[seg_idx]:\n",
    "                        scale_factor = seg_means[seg_idx] / max(pred_length, 1.0)\n",
    "                        # 保持起点不变，调整终点\n",
    "                        center_x = (Ypred_adjusted[i, x1_idx] + Ypred_adjusted[i, x2_idx]) / 2\n",
    "                        center_y = (Ypred_adjusted[i, y1_idx] + Ypred_adjusted[i, y2_idx]) / 2\n",
    "                        \n",
    "                        Ypred_adjusted[i, x1_idx] = center_x - (Ypred_adjusted[i, x2_idx] - center_x) * scale_factor\n",
    "                        Ypred_adjusted[i, y1_idx] = center_y - (Ypred_adjusted[i, y2_idx] - center_y) * scale_factor\n",
    "                        Ypred_adjusted[i, x2_idx] = center_x + (Ypred_adjusted[i, x2_idx] - center_x) * scale_factor\n",
    "                        Ypred_adjusted[i, y2_idx] = center_y + (Ypred_adjusted[i, y2_idx] - center_y) * scale_factor\n",
    "        \n",
    "        # 导出调整后的结果\n",
    "        res_df_adjusted = pd.DataFrame({\"filename\": unlabeled_names})\n",
    "        for i, col in enumerate(target_cols):\n",
    "            res_df_adjusted[col] = Ypred_adjusted[:, i] if Ypred_adjusted.shape[0] else []\n",
    "        \n",
    "        res_df_adjusted[\"seg1_label\"] = \"Medial condyle\"\n",
    "        res_df_adjusted[\"seg2_label\"] = \"Femoral condyle\"\n",
    "        res_df_adjusted[\"seg3_label\"] = \"lateral_thickness\"\n",
    "        \n",
    "        out_adjusted = OUTPUT_FILE.with_name(OUTPUT_FILE.stem + \"_adjusted.xlsx\")\n",
    "        res_df_adjusted.to_excel(out_adjusted, index=False)\n",
    "        print(\"几何约束调整后的预测结果已保存:\", out_adjusted)\n",
    "    else:\n",
    "        print(\"无法计算线段长度统计，跳过几何约束后处理\")\n",
    "else:\n",
    "    print(\"没有已标注数据，跳过几何约束后处理\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Custom)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
