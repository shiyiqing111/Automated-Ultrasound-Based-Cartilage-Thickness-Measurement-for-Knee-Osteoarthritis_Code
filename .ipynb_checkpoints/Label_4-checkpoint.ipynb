{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3ca9b8-a23e-4e56-9ac8-fbd9a627530a",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18e5ed-5574-4049-8afe-bbcaf8cee559",
   "metadata": {},
   "source": [
    "labelme C:/Users/Charlotte/Desktop/dissertation/US_new/High_quality_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6f524f3-d9e5-4f4f-bb16-54dcfc9037d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380 measurement point coordinates have been extracted and saved to: C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "json_dir = r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\Labelled_image\"\n",
    "records = []\n",
    "\n",
    "for f in os.listdir(json_dir):\n",
    "    if f.endswith(\".json\"):\n",
    "        path = os.path.join(json_dir, f)\n",
    "        with open(path, encoding=\"utf-8\") as jf:\n",
    "            data = json.load(jf)\n",
    "            for shape in data.get(\"shapes\", []):\n",
    "                pts = shape[\"points\"]\n",
    "                if len(pts) == 2:\n",
    "                    (x1, y1), (x2, y2) = pts\n",
    "                    dist = ((x1 - x2)**2 + (y1 - y2)**2)**0.5\n",
    "                    records.append({\n",
    "                        \"Filename\": f.replace(\".json\", \"\"),\n",
    "                        \"Label\": shape.get(\"label\", \"\"),\n",
    "                        \"x1\": x1, \"y1\": y1,\n",
    "                        \"x2\": x2, \"y2\": y2,\n",
    "                        \"Pixel_Distance\": dist\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "out_path = r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_summary.xlsx\"\n",
    "df.to_excel(out_path, index=False)\n",
    "print(f\"{len(df)} measurement point coordinates have been extracted and saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4df6fea2-4e4f-44c5-9dbc-02653e35fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book-format summary saved as annotation_points_summary_averaged_formatted.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_excel(\"annotation_points_summary.xlsx\")\n",
    "\n",
    "def extract_info(filename, label):\n",
    "    fn = str(filename)\n",
    "    lb = str(label).lower()\n",
    "\n",
    "    # Patient: Abbey_001 → Abbey 001\n",
    "    patient_match = re.search(r\"Abbey[_\\s]*\\d{1,3}\", fn, re.I)\n",
    "    patient = patient_match.group(0).replace(\"_\", \" \").strip() if patient_match else None\n",
    "\n",
    "    # Stage: 匹配 base 或 v3/v4/v5\n",
    "    stage_match = re.search(r\"(base|v\\d+)\", fn, re.I)\n",
    "    # Stage: 修正版\n",
    "    stage = None\n",
    "    if \"v1\" in fn.lower():\n",
    "        stage = \"base\"\n",
    "    elif \"v3\" in fn.lower():\n",
    "        stage = \"3\"\n",
    "    elif \"v4\" in fn.lower():\n",
    "        stage = \"4\"\n",
    "    elif \"v5\" in fn.lower():\n",
    "        stage = \"5\"\n",
    "    else:\n",
    "        # 兜底匹配 base\n",
    "        if \"base\" in fn.lower():\n",
    "            stage = \"base\"\n",
    "\n",
    "\n",
    "    # Location\n",
    "    location = \"contralateral\" if \"contralateral\" in fn.lower() else (\"treat\" if \"treat\" in fn.lower() else None)\n",
    "\n",
    "    # Part from Label\n",
    "    if \"medial\" in lb:\n",
    "        part = \"medial\"\n",
    "    elif \"femoral\" in lb:\n",
    "        part = \"femoral\"\n",
    "    elif \"lateral\" in lb:\n",
    "        part = \"lateral\"\n",
    "    else:\n",
    "        part = None\n",
    "\n",
    "    return patient, stage, location, part\n",
    "\n",
    "df[[\"Patient\", \"Stage\", \"Location\", \"Part\"]] = df.apply(lambda x: pd.Series(extract_info(x[\"Filename\"], x[\"Label\"])), axis=1)\n",
    "\n",
    "grouped = df.groupby([\"Patient\", \"Stage\", \"Location\", \"Part\"])[\"Pixel_Distance\"].mean().reset_index()\n",
    "\n",
    "pivot = grouped.pivot_table(index=\"Patient\", columns=[\"Part\",\"Location\",\"Stage\"], values=\"Pixel_Distance\")\n",
    "pivot.columns = [f\"US_{p}_{l}_{s}\" for (p,l,s) in pivot.columns]\n",
    "pivot.reset_index(inplace=True)\n",
    "\n",
    "cols = [\"Patient\",\n",
    "    \"US_medial_treat_base\",\"US_femoral_treat_base\",\"US_lateral_treat_base\",\n",
    "    \"US_medial_contralateral_base\",\"US_femoral_contralateral_base\",\"US_lateral_contralateral_base\",\n",
    "    \"US_medial_treat_3\",\"US_femoral_treat_3\",\"US_lateral_treat_3\",\n",
    "    \"US_medial_contralateral_3\",\"US_femoral_contralateral_3\",\"US_lateral_contralateral_3\",\n",
    "    \"US_medial_treat_4\",\"US_femoral_treat_4\",\"US_lateral_treat_4\",\n",
    "    \"US_medial_contralateral_4\",\"US_femoral_contralateral_4\",\"US_lateral_contralateral_4\",\n",
    "    \"US_medial_treat_5\",\"US_femoral_treat_5\",\"US_lateral_treat_5\",\n",
    "    \"US_medial_contralateral_5\",\"US_femoral_contralateral_5\",\"US_lateral_contralateral_5\"\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    if c not in pivot.columns:\n",
    "        pivot[c] = None\n",
    "pivot = pivot[cols]\n",
    "\n",
    "pivot.to_excel(\"annotation_points_summary_averaged_formatted.xlsx\", index=False)\n",
    "print(\"Book-format summary saved as annotation_points_summary_averaged_formatted.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2298c3f9-f913-4470-b114-3dade89898a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized ratio: 0.003985\n",
      "Pairs used: 402\n",
      "Error mean: -0.008319\n",
      "Error std: 0.055875\n",
      "Lower bound (mean-2SD): -0.12007\n",
      "Upper bound (mean+2SD): 0.103432\n",
      "Abnormal count: 18\n",
      "Abnormal samples (first 18):\n",
      "Patient: Abbey 009 | Measure: US_femoral_treat_base | Pred(cm): 0.3799 | Book(cm): 0.25 | Error: 0.1299\n",
      "Patient: Abbey 003 | Measure: US_medial_contralateral_base | Pred(cm): 0.0613 | Book(cm): 0.22 | Error: -0.1587\n",
      "Patient: Abbey 011 | Measure: US_medial_contralateral_base | Pred(cm): 0.1516 | Book(cm): 0.04 | Error: 0.1116\n",
      "Patient: Abbey 009 | Measure: US_femoral_contralateral_base | Pred(cm): 0.3419 | Book(cm): 0.21 | Error: 0.1319\n",
      "Patient: Abbey 018 | Measure: US_femoral_contralateral_base | Pred(cm): 0.4293 | Book(cm): 0.19 | Error: 0.2393\n",
      "Patient: Abbey 010 | Measure: US_medial_treat_3 | Pred(cm): 0.075 | Book(cm): 0.2 | Error: -0.125\n",
      "Patient: Abbey 011 | Measure: US_femoral_treat_3 | Pred(cm): 0.2104 | Book(cm): 0.08 | Error: 0.1304\n",
      "Patient: Abbey 009 | Measure: US_femoral_contralateral_3 | Pred(cm): 0.3325 | Book(cm): 0.21 | Error: 0.1225\n",
      "Patient: Abbey 011 | Measure: US_lateral_contralateral_3 | Pred(cm): 0.3701 | Book(cm): 0.14 | Error: 0.2301\n",
      "Patient: Abbey 009 | Measure: US_medial_treat_4 | Pred(cm): 0.2878 | Book(cm): 0.18 | Error: 0.1078\n",
      "Patient: Abbey 012 | Measure: US_femoral_treat_4 | Pred(cm): 0.3678 | Book(cm): 0.16 | Error: 0.2078\n",
      "Patient: Abbey 003 | Measure: US_lateral_contralateral_4 | Pred(cm): 0.0758 | Book(cm): 0.2 | Error: -0.1242\n",
      "Patient: Abbey 007 | Measure: US_lateral_contralateral_4 | Pred(cm): 0.2641 | Book(cm): 0.16 | Error: 0.1041\n",
      "Patient: Abbey 007 | Measure: US_femoral_treat_5 | Pred(cm): 0.1265 | Book(cm): 0.28 | Error: -0.1535\n",
      "Patient: Abbey 007 | Measure: US_lateral_treat_5 | Pred(cm): 0.1124 | Book(cm): 0.24 | Error: -0.1276\n",
      "Patient: Abbey 003 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.05 | Book(cm): 0.2 | Error: -0.15\n",
      "Patient: Abbey 005 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.0512 | Book(cm): 0.19 | Error: -0.1388\n",
      "Patient: Abbey 007 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.3075 | Book(cm): 0.16 | Error: 0.1475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_book = pd.read_excel('Book2.xlsx')\n",
    "df_ann = pd.read_excel('annotation_points_summary_averaged_formatted.xlsx')\n",
    "\n",
    "us_cols_book = [c for c in df_book.columns if c.startswith('US_')]\n",
    "us_cols_ann = [c for c in df_ann.columns if c.startswith('US_')]\n",
    "overlap = [c for c in us_cols_book if c in us_cols_ann]\n",
    "\n",
    "merged = pd.merge(df_book[['Patient'] + overlap], df_ann[['Patient'] + overlap], on='Patient', how='inner', suffixes=('_book', '_ann'))\n",
    "\n",
    "pairs_true = []\n",
    "pairs_pred = []\n",
    "for col in overlap:\n",
    "    tb = f'{col}_book'\n",
    "    ta = f'{col}_ann'\n",
    "    sub = merged[[tb, ta]].dropna()\n",
    "    if not sub.empty:\n",
    "        pairs_true.append(sub[tb].values.astype(float))\n",
    "        pairs_pred.append(sub[ta].values.astype(float))\n",
    "if len(pairs_true) == 0:\n",
    "    raise ValueError('No valid overlapping measurements to optimize on.')\n",
    "y_true = np.concatenate(pairs_true)\n",
    "x_pred = np.concatenate(pairs_pred)\n",
    "\n",
    "ratios = np.linspace(0.001, 0.1, 200)\n",
    "best_ratio = None\n",
    "best_mae = float('inf')\n",
    "for r in ratios:\n",
    "    mae = np.mean(np.abs(y_true - x_pred * r))\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_ratio = r\n",
    "\n",
    "df_ann_scaled = df_ann.copy()\n",
    "for col in overlap:\n",
    "    if col in df_ann_scaled.columns:\n",
    "        df_ann_scaled[col] = df_ann_scaled[col].astype(float) * best_ratio\n",
    "\n",
    "merged_scaled = pd.merge(df_book[['Patient'] + overlap], df_ann_scaled[['Patient'] + overlap], on='Patient', how='inner', suffixes=('_book', '_ann'))\n",
    "\n",
    "errs = []\n",
    "rows = []\n",
    "for col in overlap:\n",
    "    tb = f'{col}_book'\n",
    "    ta = f'{col}_ann'\n",
    "    sub = merged_scaled[['Patient', tb, ta]].dropna()\n",
    "    if not sub.empty:\n",
    "        e = sub[ta].values.astype(float) - sub[tb].values.astype(float)\n",
    "        errs.append(e)\n",
    "        for i in range(len(sub)):\n",
    "            rows.append((sub.iloc[i]['Patient'], col, float(sub.iloc[i][ta]), float(sub.iloc[i][tb]), float(e[i])))\n",
    "if len(errs) == 0:\n",
    "    raise ValueError('No valid pairs after scaling.')\n",
    "errs_all = np.concatenate(errs)\n",
    "mu = float(np.mean(errs_all))\n",
    "sd = float(np.std(errs_all))\n",
    "lower = mu - 2.0 * sd\n",
    "upper = mu + 2.0 * sd\n",
    "\n",
    "abnormal = []\n",
    "for patient, col, pred_cm, book_cm, e in rows:\n",
    "    if e < lower or e > upper:\n",
    "        abnormal.append((patient, col, pred_cm, book_cm, e))\n",
    "\n",
    "df_ann_scaled.to_excel('annotation_points_summary_averaged_formatted.xlsx', index=False)\n",
    "\n",
    "print('Optimized ratio:', round(best_ratio, 6))\n",
    "print('Pairs used:', len(errs_all))\n",
    "print('Error mean:', round(mu, 6))\n",
    "print('Error std:', round(sd, 6))\n",
    "print('Lower bound (mean-2SD):', round(lower, 6))\n",
    "print('Upper bound (mean+2SD):', round(upper, 6))\n",
    "print('Abnormal count:', len(abnormal))\n",
    "if len(abnormal) > 0:\n",
    "    head = min(50, len(abnormal))\n",
    "    print('Abnormal samples (first {}):'.format(head))\n",
    "    for i in range(head):\n",
    "        p, c, pv, bv, e = abnormal[i]\n",
    "        print('Patient:', p, '| Measure:', c, '| Pred(cm):', round(pv, 4), '| Book(cm):', round(bv, 4), '| Error:', round(e, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7418b-eb51-4e0c-b36a-8b382f7efb5a",
   "metadata": {},
   "source": [
    "## re-Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37f6e9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f26a96-6f4a-4b18-8bd9-96fcae601602",
   "metadata": {},
   "source": [
    "labelme C:/Users/Charlotte/Desktop/dissertation/US_new/High_quality_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b571ec-9061-4285-ad82-a4d9ecfe8c6e",
   "metadata": {},
   "source": [
    "应该是Abbey 13 - Patient: Abbey 009 | Measure: US_femoral_treat_base （删掉）\n",
    "Patient: Abbey 003 | Measure: US_medial_contralateral_base （修改）\n",
    "Patient: Abbey 011 | Measure: US_medial_contralateral_base （修改）\n",
    "Patient: Abbey 009 | Measure: US_femoral_contralateral_base（修改）\n",
    "Patient: Abbey 018 | Measure: US_femoral_contralateral_base（修改）\n",
    "Patient: Abbey 010 | Measure: US_medial_treat_3（修改）\n",
    "Patient: Abbey 011 | Measure: US_femoral_treat_3（修改）\n",
    "Patient: Abbey 009 | Measure: US_femoral_contralateral_3（修改）\n",
    "Patient: Abbey 011 | Measure: US_lateral_contralateral_3（修改）\n",
    "Patient: Abbey 009 | Measure: US_medial_treat_4（修改）\n",
    "Patient: Abbey 012 | Measure: US_femoral_treat_4（修改）\n",
    "Patient: Abbey 003 | Measure: US_lateral_contralateral_4（修改）\n",
    "Patient: Abbey 007 | Measure: US_lateral_contralateral_4（修改）\n",
    "Patient: Abbey 007 | Measure: US_femoral_treat_5（修改）\n",
    "Patient: Abbey 007 | Measure: US_lateral_treat_5（修改）\n",
    "Patient: Abbey 003 | Measure: US_lateral_contralateral_5（修改）\n",
    "Patient: Abbey 005 | Measure: US_lateral_contralateral_5（修改）\n",
    "Patient: Abbey 007 | Measure: US_lateral_contralateral_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfdeab",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8fc661b-8e66-47a1-8391-16e161664f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372 measurement point coordinates have been extracted and saved to: C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_summary2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "json_dir = r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\Labelled_image\"\n",
    "records = []\n",
    "\n",
    "for f in os.listdir(json_dir):\n",
    "    if f.endswith(\".json\"):\n",
    "        path = os.path.join(json_dir, f)\n",
    "        with open(path, encoding=\"utf-8\") as jf:\n",
    "            data = json.load(jf)\n",
    "            for shape in data.get(\"shapes\", []):\n",
    "                pts = shape[\"points\"]\n",
    "                if len(pts) == 2:\n",
    "                    (x1, y1), (x2, y2) = pts\n",
    "                    dist = ((x1 - x2)**2 + (y1 - y2)**2)**0.5\n",
    "                    records.append({\n",
    "                        \"Filename\": f.replace(\".json\", \"\"),\n",
    "                        \"Label\": shape.get(\"label\", \"\"),\n",
    "                        \"x1\": x1, \"y1\": y1,\n",
    "                        \"x2\": x2, \"y2\": y2,\n",
    "                        \"Pixel_Distance\": dist\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "out_path = r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_summary2.xlsx\"\n",
    "df.to_excel(out_path, index=False)\n",
    "print(f\"{len(df)} measurement point coordinates have been extracted and saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8df3550a-99be-4181-8f1d-07eb0df04b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book-format summary saved as annotation_points_summary_averaged_formatted2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_excel(\"annotation_points_summary2.xlsx\")\n",
    "\n",
    "def extract_info(filename, label):\n",
    "    fn = str(filename)\n",
    "    lb = str(label).lower()\n",
    "\n",
    "    # Patient: Abbey_001 → Abbey 001\n",
    "    patient_match = re.search(r\"Abbey[_\\s]*\\d{1,3}\", fn, re.I)\n",
    "    patient = patient_match.group(0).replace(\"_\", \" \").strip() if patient_match else None\n",
    "\n",
    "    # Stage: 匹配 base 或 v3/v4/v5\n",
    "    stage_match = re.search(r\"(base|v\\d+)\", fn, re.I)\n",
    "    # Stage: 修正版\n",
    "    stage = None\n",
    "    if \"v1\" in fn.lower():\n",
    "        stage = \"base\"\n",
    "    elif \"v3\" in fn.lower():\n",
    "        stage = \"3\"\n",
    "    elif \"v4\" in fn.lower():\n",
    "        stage = \"4\"\n",
    "    elif \"v5\" in fn.lower():\n",
    "        stage = \"5\"\n",
    "    else:\n",
    "        # 兜底匹配 base\n",
    "        if \"base\" in fn.lower():\n",
    "            stage = \"base\"\n",
    "\n",
    "\n",
    "    # Location\n",
    "    location = \"contralateral\" if \"contralateral\" in fn.lower() else (\"treat\" if \"treat\" in fn.lower() else None)\n",
    "\n",
    "    # Part from Label\n",
    "    if \"medial\" in lb:\n",
    "        part = \"medial\"\n",
    "    elif \"femoral\" in lb:\n",
    "        part = \"femoral\"\n",
    "    elif \"lateral\" in lb:\n",
    "        part = \"lateral\"\n",
    "    else:\n",
    "        part = None\n",
    "\n",
    "    return patient, stage, location, part\n",
    "\n",
    "df[[\"Patient\", \"Stage\", \"Location\", \"Part\"]] = df.apply(lambda x: pd.Series(extract_info(x[\"Filename\"], x[\"Label\"])), axis=1)\n",
    "\n",
    "grouped = df.groupby([\"Patient\", \"Stage\", \"Location\", \"Part\"])[\"Pixel_Distance\"].mean().reset_index()\n",
    "\n",
    "pivot = grouped.pivot_table(index=\"Patient\", columns=[\"Part\",\"Location\",\"Stage\"], values=\"Pixel_Distance\")\n",
    "pivot.columns = [f\"US_{p}_{l}_{s}\" for (p,l,s) in pivot.columns]\n",
    "pivot.reset_index(inplace=True)\n",
    "\n",
    "cols = [\"Patient\",\n",
    "    \"US_medial_treat_base\",\"US_femoral_treat_base\",\"US_lateral_treat_base\",\n",
    "    \"US_medial_contralateral_base\",\"US_femoral_contralateral_base\",\"US_lateral_contralateral_base\",\n",
    "    \"US_medial_treat_3\",\"US_femoral_treat_3\",\"US_lateral_treat_3\",\n",
    "    \"US_medial_contralateral_3\",\"US_femoral_contralateral_3\",\"US_lateral_contralateral_3\",\n",
    "    \"US_medial_treat_4\",\"US_femoral_treat_4\",\"US_lateral_treat_4\",\n",
    "    \"US_medial_contralateral_4\",\"US_femoral_contralateral_4\",\"US_lateral_contralateral_4\",\n",
    "    \"US_medial_treat_5\",\"US_femoral_treat_5\",\"US_lateral_treat_5\",\n",
    "    \"US_medial_contralateral_5\",\"US_femoral_contralateral_5\",\"US_lateral_contralateral_5\"\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    if c not in pivot.columns:\n",
    "        pivot[c] = None\n",
    "pivot = pivot[cols]\n",
    "\n",
    "pivot.to_excel(\"annotation_points_summary_averaged_formatted2.xlsx\", index=False)\n",
    "print(\"Book-format summary saved as annotation_points_summary_averaged_formatted2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45c533bf-a1a8-4fa6-838b-9549a7c2da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized ratio: 0.003985\n",
      "Pairs used: 399\n",
      "Error mean: -0.009972\n",
      "Error std: 0.050526\n",
      "Lower bound (mean-2SD): -0.111023\n",
      "Upper bound (mean+2SD): 0.09108\n",
      "Abnormal count: 13\n",
      "Abnormal samples (first 13):\n",
      "Patient: Abbey 003 | Measure: US_femoral_contralateral_base | Pred(cm): 0.3161 | Book(cm): 0.21 | Error: 0.1061\n",
      "Patient: Abbey 011 | Measure: US_medial_contralateral_3 | Pred(cm): 0.1318 | Book(cm): 0.04 | Error: 0.0918\n",
      "Patient: Abbey 009 | Measure: US_medial_treat_4 | Pred(cm): 0.2825 | Book(cm): 0.18 | Error: 0.1025\n",
      "Patient: Abbey 012 | Measure: US_femoral_treat_4 | Pred(cm): 0.3678 | Book(cm): 0.16 | Error: 0.2078\n",
      "Patient: Abbey 003 | Measure: US_lateral_treat_4 | Pred(cm): 0.1159 | Book(cm): 0.23 | Error: -0.1141\n",
      "Patient: Abbey 009 | Measure: US_lateral_treat_4 | Pred(cm): 0.2799 | Book(cm): 0.18 | Error: 0.0999\n",
      "Patient: Abbey 007 | Measure: US_femoral_treat_5 | Pred(cm): 0.1351 | Book(cm): 0.28 | Error: -0.1449\n",
      "Patient: Abbey 011 | Measure: US_femoral_treat_5 | Pred(cm): 0.1703 | Book(cm): 0.07 | Error: 0.1003\n",
      "Patient: Abbey 007 | Measure: US_lateral_treat_5 | Pred(cm): 0.1014 | Book(cm): 0.24 | Error: -0.1386\n",
      "Patient: Abbey 016 | Measure: US_medial_contralateral_5 | Pred(cm): 0.1856 | Book(cm): 0.09 | Error: 0.0956\n",
      "Patient: Abbey 003 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.0485 | Book(cm): 0.2 | Error: -0.1515\n",
      "Patient: Abbey 005 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.0637 | Book(cm): 0.19 | Error: -0.1263\n",
      "Patient: Abbey 012 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.1635 | Book(cm): 0.07 | Error: 0.0935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_book = pd.read_excel('Book2.xlsx')\n",
    "df_ann = pd.read_excel('annotation_points_summary_averaged_formatted2.xlsx')\n",
    "\n",
    "us_cols_book = [c for c in df_book.columns if c.startswith('US_')]\n",
    "us_cols_ann = [c for c in df_ann.columns if c.startswith('US_')]\n",
    "overlap = [c for c in us_cols_book if c in us_cols_ann]\n",
    "\n",
    "merged = pd.merge(df_book[['Patient'] + overlap], df_ann[['Patient'] + overlap], on='Patient', how='inner', suffixes=('_book', '_ann'))\n",
    "\n",
    "pairs_true = []\n",
    "pairs_pred = []\n",
    "for col in overlap:\n",
    "    tb = f'{col}_book'\n",
    "    ta = f'{col}_ann'\n",
    "    sub = merged[[tb, ta]].dropna()\n",
    "    if not sub.empty:\n",
    "        pairs_true.append(sub[tb].values.astype(float))\n",
    "        pairs_pred.append(sub[ta].values.astype(float))\n",
    "if len(pairs_true) == 0:\n",
    "    raise ValueError('No valid overlapping measurements to optimize on.')\n",
    "y_true = np.concatenate(pairs_true)\n",
    "x_pred = np.concatenate(pairs_pred)\n",
    "\n",
    "ratios = np.linspace(0.001, 0.1, 200)\n",
    "best_ratio = None\n",
    "best_mae = float('inf')\n",
    "for r in ratios:\n",
    "    mae = np.mean(np.abs(y_true - x_pred * r))\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_ratio = r\n",
    "\n",
    "df_ann_scaled = df_ann.copy()\n",
    "for col in overlap:\n",
    "    if col in df_ann_scaled.columns:\n",
    "        df_ann_scaled[col] = df_ann_scaled[col].astype(float) * best_ratio\n",
    "\n",
    "merged_scaled = pd.merge(df_book[['Patient'] + overlap], df_ann_scaled[['Patient'] + overlap], on='Patient', how='inner', suffixes=('_book', '_ann'))\n",
    "\n",
    "errs = []\n",
    "rows = []\n",
    "for col in overlap:\n",
    "    tb = f'{col}_book'\n",
    "    ta = f'{col}_ann'\n",
    "    sub = merged_scaled[['Patient', tb, ta]].dropna()\n",
    "    if not sub.empty:\n",
    "        e = sub[ta].values.astype(float) - sub[tb].values.astype(float)\n",
    "        errs.append(e)\n",
    "        for i in range(len(sub)):\n",
    "            rows.append((sub.iloc[i]['Patient'], col, float(sub.iloc[i][ta]), float(sub.iloc[i][tb]), float(e[i])))\n",
    "if len(errs) == 0:\n",
    "    raise ValueError('No valid pairs after scaling.')\n",
    "errs_all = np.concatenate(errs)\n",
    "mu = float(np.mean(errs_all))\n",
    "sd = float(np.std(errs_all))\n",
    "lower = mu - 2.0 * sd\n",
    "upper = mu + 2.0 * sd\n",
    "\n",
    "abnormal = []\n",
    "for patient, col, pred_cm, book_cm, e in rows:\n",
    "    if e < lower or e > upper:\n",
    "        abnormal.append((patient, col, pred_cm, book_cm, e))\n",
    "\n",
    "df_ann_scaled.to_excel('annotation_points_summary_averaged_formatted2.xlsx', index=False)\n",
    "\n",
    "print('Optimized ratio:', round(best_ratio, 6))\n",
    "print('Pairs used:', len(errs_all))\n",
    "print('Error mean:', round(mu, 6))\n",
    "print('Error std:', round(sd, 6))\n",
    "print('Lower bound (mean-2SD):', round(lower, 6))\n",
    "print('Upper bound (mean+2SD):', round(upper, 6))\n",
    "print('Abnormal count:', len(abnormal))\n",
    "if len(abnormal) > 0:\n",
    "    head = min(50, len(abnormal))\n",
    "    print('Abnormal samples (first {}):'.format(head))\n",
    "    for i in range(head):\n",
    "        p, c, pv, bv, e = abnormal[i]\n",
    "        print('Patient:', p, '| Measure:', c, '| Pred(cm):', round(pv, 4), '| Book(cm):', round(bv, 4), '| Error:', round(e, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1eead-584d-4fe4-a004-f0fad9e2a8f9",
   "metadata": {},
   "source": [
    "## re-re-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8840a0-8eae-4a0f-a210-19ad11c8ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Patient: Abbey 003 | Measure: US_femoral_contralateral_base *\n",
    "Patient: Abbey 011 | Measure: US_medial_contralateral_3 *\n",
    "Patient: Abbey 009 | Measure: US_medial_treat_4 *\n",
    "Patient: Abbey 012 | Measure: US_femoral_treat_4 *\n",
    "Patient: Abbey 003 | Measure: US_lateral_treat_4 *\n",
    "Patient: Abbey 009 | Measure: US_lateral_treat_4 *\n",
    "Patient: Abbey 007 | Measure: US_femoral_treat_5 *\n",
    "Patient: Abbey 011 | Measure: US_femoral_treat_5 * \n",
    "Patient: Abbey 007 | Measure: US_lateral_treat_5 *\n",
    "Patient: Abbey 016 | Measure: US_medial_contralateral_5 *\n",
    "Patient: Abbey 003 | Measure: US_lateral_contralateral_5 EG\n",
    "Patient: Abbey 005 | Measure: US_lateral_contralateral_5 *\n",
    "Patient: Abbey 012 | Measure: US_lateral_contralateral_5 *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd226e70-94ec-46dd-832f-1e30ee05d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374 measurement point coordinates have been extracted and saved to: C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_summary2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "json_dir = r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\Labelled_image\"\n",
    "records = []\n",
    "\n",
    "for f in os.listdir(json_dir):\n",
    "    if f.endswith(\".json\"):\n",
    "        path = os.path.join(json_dir, f)\n",
    "        with open(path, encoding=\"utf-8\") as jf:\n",
    "            data = json.load(jf)\n",
    "            for shape in data.get(\"shapes\", []):\n",
    "                pts = shape[\"points\"]\n",
    "                if len(pts) == 2:\n",
    "                    (x1, y1), (x2, y2) = pts\n",
    "                    dist = ((x1 - x2)**2 + (y1 - y2)**2)**0.5\n",
    "                    records.append({\n",
    "                        \"Filename\": f.replace(\".json\", \"\"),\n",
    "                        \"Label\": shape.get(\"label\", \"\"),\n",
    "                        \"x1\": x1, \"y1\": y1,\n",
    "                        \"x2\": x2, \"y2\": y2,\n",
    "                        \"Pixel_Distance\": dist\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "out_path = r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\annotation_points_summary2.xlsx\"\n",
    "df.to_excel(out_path, index=False)\n",
    "print(f\"{len(df)} measurement point coordinates have been extracted and saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1550ea7c-a9d5-420d-917e-663ed3438e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book-format summary saved as annotation_points_summary_averaged_formatted2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_excel(\"annotation_points_summary2.xlsx\")\n",
    "\n",
    "def extract_info(filename, label):\n",
    "    fn = str(filename)\n",
    "    lb = str(label).lower()\n",
    "\n",
    "    # Patient: Abbey_001 → Abbey 001\n",
    "    patient_match = re.search(r\"Abbey[_\\s]*\\d{1,3}\", fn, re.I)\n",
    "    patient = patient_match.group(0).replace(\"_\", \" \").strip() if patient_match else None\n",
    "\n",
    "    # Stage: 匹配 base 或 v3/v4/v5\n",
    "    stage_match = re.search(r\"(base|v\\d+)\", fn, re.I)\n",
    "    # Stage: 修正版\n",
    "    stage = None\n",
    "    if \"v1\" in fn.lower():\n",
    "        stage = \"base\"\n",
    "    elif \"v3\" in fn.lower():\n",
    "        stage = \"3\"\n",
    "    elif \"v4\" in fn.lower():\n",
    "        stage = \"4\"\n",
    "    elif \"v5\" in fn.lower():\n",
    "        stage = \"5\"\n",
    "    else:\n",
    "        # 兜底匹配 base\n",
    "        if \"base\" in fn.lower():\n",
    "            stage = \"base\"\n",
    "\n",
    "\n",
    "    # Location\n",
    "    location = \"contralateral\" if \"contralateral\" in fn.lower() else (\"treat\" if \"treat\" in fn.lower() else None)\n",
    "\n",
    "    # Part from Label\n",
    "    if \"medial\" in lb:\n",
    "        part = \"medial\"\n",
    "    elif \"femoral\" in lb:\n",
    "        part = \"femoral\"\n",
    "    elif \"lateral\" in lb:\n",
    "        part = \"lateral\"\n",
    "    else:\n",
    "        part = None\n",
    "\n",
    "    return patient, stage, location, part\n",
    "\n",
    "df[[\"Patient\", \"Stage\", \"Location\", \"Part\"]] = df.apply(lambda x: pd.Series(extract_info(x[\"Filename\"], x[\"Label\"])), axis=1)\n",
    "\n",
    "grouped = df.groupby([\"Patient\", \"Stage\", \"Location\", \"Part\"])[\"Pixel_Distance\"].mean().reset_index()\n",
    "\n",
    "pivot = grouped.pivot_table(index=\"Patient\", columns=[\"Part\",\"Location\",\"Stage\"], values=\"Pixel_Distance\")\n",
    "pivot.columns = [f\"US_{p}_{l}_{s}\" for (p,l,s) in pivot.columns]\n",
    "pivot.reset_index(inplace=True)\n",
    "\n",
    "cols = [\"Patient\",\n",
    "    \"US_medial_treat_base\",\"US_femoral_treat_base\",\"US_lateral_treat_base\",\n",
    "    \"US_medial_contralateral_base\",\"US_femoral_contralateral_base\",\"US_lateral_contralateral_base\",\n",
    "    \"US_medial_treat_3\",\"US_femoral_treat_3\",\"US_lateral_treat_3\",\n",
    "    \"US_medial_contralateral_3\",\"US_femoral_contralateral_3\",\"US_lateral_contralateral_3\",\n",
    "    \"US_medial_treat_4\",\"US_femoral_treat_4\",\"US_lateral_treat_4\",\n",
    "    \"US_medial_contralateral_4\",\"US_femoral_contralateral_4\",\"US_lateral_contralateral_4\",\n",
    "    \"US_medial_treat_5\",\"US_femoral_treat_5\",\"US_lateral_treat_5\",\n",
    "    \"US_medial_contralateral_5\",\"US_femoral_contralateral_5\",\"US_lateral_contralateral_5\"\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    if c not in pivot.columns:\n",
    "        pivot[c] = None\n",
    "pivot = pivot[cols]\n",
    "\n",
    "pivot.to_excel(\"annotation_points_summary_averaged_formatted2.xlsx\", index=False)\n",
    "print(\"Book-format summary saved as annotation_points_summary_averaged_formatted2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31429626-0a45-4379-96ad-906f43a9cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized ratio: 0.003985\n",
      "Pairs used: 399\n",
      "Error mean: -0.011181\n",
      "Error std: 0.048017\n",
      "Lower bound (mean-2SD): -0.107216\n",
      "Upper bound (mean+2SD): 0.084854\n",
      "Abnormal count: 12\n",
      "Abnormal samples (first 12):\n",
      "Patient: Abbey 005 | Measure: US_medial_contralateral_base | Pred(cm): 0.2478 | Book(cm): 0.16 | Error: 0.0878\n",
      "Patient: Abbey 012 | Measure: US_medial_contralateral_base | Pred(cm): 0.1977 | Book(cm): 0.11 | Error: 0.0877\n",
      "Patient: Abbey 009 | Measure: US_femoral_contralateral_base | Pred(cm): 0.3002 | Book(cm): 0.21 | Error: 0.0902\n",
      "Patient: Abbey 007 | Measure: US_femoral_treat_3 | Pred(cm): 0.1713 | Book(cm): 0.28 | Error: -0.1087\n",
      "Patient: Abbey 007 | Measure: US_medial_contralateral_3 | Pred(cm): 0.0802 | Book(cm): 0.19 | Error: -0.1098\n",
      "Patient: Abbey 009 | Measure: US_lateral_contralateral_3 | Pred(cm): 0.2352 | Book(cm): 0.15 | Error: 0.0852\n",
      "Patient: Abbey 012 | Measure: US_femoral_treat_4 | Pred(cm): 0.3498 | Book(cm): 0.16 | Error: 0.1898\n",
      "Patient: Abbey 018 | Measure: US_femoral_treat_4 | Pred(cm): 0.1669 | Book(cm): 0.08 | Error: 0.0869\n",
      "Patient: Abbey 008 | Measure: US_lateral_contralateral_4 | Pred(cm): 0.1127 | Book(cm): 0.22 | Error: -0.1073\n",
      "Patient: Abbey 007 | Measure: US_lateral_treat_5 | Pred(cm): 0.1009 | Book(cm): 0.24 | Error: -0.1391\n",
      "Patient: Abbey 003 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.0485 | Book(cm): 0.2 | Error: -0.1515\n",
      "Patient: Abbey 005 | Measure: US_lateral_contralateral_5 | Pred(cm): 0.0657 | Book(cm): 0.19 | Error: -0.1243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_book = pd.read_excel('Book2.xlsx')\n",
    "df_ann = pd.read_excel('annotation_points_summary_averaged_formatted2.xlsx')\n",
    "\n",
    "us_cols_book = [c for c in df_book.columns if c.startswith('US_')]\n",
    "us_cols_ann = [c for c in df_ann.columns if c.startswith('US_')]\n",
    "overlap = [c for c in us_cols_book if c in us_cols_ann]\n",
    "\n",
    "merged = pd.merge(df_book[['Patient'] + overlap], df_ann[['Patient'] + overlap], on='Patient', how='inner', suffixes=('_book', '_ann'))\n",
    "\n",
    "pairs_true = []\n",
    "pairs_pred = []\n",
    "for col in overlap:\n",
    "    tb = f'{col}_book'\n",
    "    ta = f'{col}_ann'\n",
    "    sub = merged[[tb, ta]].dropna()\n",
    "    if not sub.empty:\n",
    "        pairs_true.append(sub[tb].values.astype(float))\n",
    "        pairs_pred.append(sub[ta].values.astype(float))\n",
    "if len(pairs_true) == 0:\n",
    "    raise ValueError('No valid overlapping measurements to optimize on.')\n",
    "y_true = np.concatenate(pairs_true)\n",
    "x_pred = np.concatenate(pairs_pred)\n",
    "\n",
    "ratios = np.linspace(0.001, 0.1, 200)\n",
    "best_ratio = None\n",
    "best_mae = float('inf')\n",
    "for r in ratios:\n",
    "    mae = np.mean(np.abs(y_true - x_pred * r))\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_ratio = r\n",
    "\n",
    "df_ann_scaled = df_ann.copy()\n",
    "for col in overlap:\n",
    "    if col in df_ann_scaled.columns:\n",
    "        df_ann_scaled[col] = df_ann_scaled[col].astype(float) * best_ratio\n",
    "\n",
    "merged_scaled = pd.merge(df_book[['Patient'] + overlap], df_ann_scaled[['Patient'] + overlap], on='Patient', how='inner', suffixes=('_book', '_ann'))\n",
    "\n",
    "errs = []\n",
    "rows = []\n",
    "for col in overlap:\n",
    "    tb = f'{col}_book'\n",
    "    ta = f'{col}_ann'\n",
    "    sub = merged_scaled[['Patient', tb, ta]].dropna()\n",
    "    if not sub.empty:\n",
    "        e = sub[ta].values.astype(float) - sub[tb].values.astype(float)\n",
    "        errs.append(e)\n",
    "        for i in range(len(sub)):\n",
    "            rows.append((sub.iloc[i]['Patient'], col, float(sub.iloc[i][ta]), float(sub.iloc[i][tb]), float(e[i])))\n",
    "if len(errs) == 0:\n",
    "    raise ValueError('No valid pairs after scaling.')\n",
    "errs_all = np.concatenate(errs)\n",
    "mu = float(np.mean(errs_all))\n",
    "sd = float(np.std(errs_all))\n",
    "lower = mu - 2.0 * sd\n",
    "upper = mu + 2.0 * sd\n",
    "\n",
    "abnormal = []\n",
    "for patient, col, pred_cm, book_cm, e in rows:\n",
    "    if e < lower or e > upper:\n",
    "        abnormal.append((patient, col, pred_cm, book_cm, e))\n",
    "\n",
    "df_ann_scaled.to_excel('annotation_points_summary_averaged_formatted2.xlsx', index=False)\n",
    "\n",
    "print('Optimized ratio:', round(best_ratio, 6))\n",
    "print('Pairs used:', len(errs_all))\n",
    "print('Error mean:', round(mu, 6))\n",
    "print('Error std:', round(sd, 6))\n",
    "print('Lower bound (mean-2SD):', round(lower, 6))\n",
    "print('Upper bound (mean+2SD):', round(upper, 6))\n",
    "print('Abnormal count:', len(abnormal))\n",
    "if len(abnormal) > 0:\n",
    "    head = min(50, len(abnormal))\n",
    "    print('Abnormal samples (first {}):'.format(head))\n",
    "    for i in range(head):\n",
    "        p, c, pv, bv, e = abnormal[i]\n",
    "        print('Patient:', p, '| Measure:', c, '| Pred(cm):', round(pv, 4), '| Book(cm):', round(bv, 4), '| Error:', round(e, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ff364a-71c2-4a90-8385-5a6eefbc0570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已重命名 1179 个文件:\n",
      " - Abbey 001_v1_contralateral_base (1).jpg  →  Abbey_001_v1_contralateral_base_001.jpg\n",
      " - Abbey 001_v1_contralateral_base (4).jpg  →  Abbey_001_v1_contralateral_base_004.jpg\n",
      " - Abbey 001_v1_treat_base (6).jpg  →  Abbey_001_v1_treat_base_006.jpg\n",
      " - Abbey 001_v1_treat_base (7).jpg  →  Abbey_001_v1_treat_base_007.jpg\n",
      " - Abbey 001_v3_contralateral_3 (1).jpg  →  Abbey_001_v3_contralateral_003_1.jpg\n",
      " - Abbey 001_v3_contralateral_3 (4).jpg  →  Abbey_001_v3_contralateral_003_4.jpg\n",
      " - Abbey 001_v3_contralateral_3 (5).jpg  →  Abbey_001_v3_contralateral_003_5.jpg\n",
      " - Abbey 001_v3_treat_3 (11).jpg  →  Abbey_001_v3_treat_003_11.jpg\n",
      " - Abbey 001_v3_treat_3 (12).jpg  →  Abbey_001_v3_treat_003_12.jpg\n",
      " - Abbey 001_v3_treat_3 (7).jpg  →  Abbey_001_v3_treat_003_7.jpg\n",
      " ... 共 1179 个文件已更新。\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "root = Path(r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\High_quality_images\")\n",
    "def normalize_filename(name: str) -> str:\n",
    "    \"\"\"\n",
    "    将:\n",
    "        'Abbey 001_v1_contralateral_base (1).jpg'\n",
    "    统一成:\n",
    "        'Abbey_001_v1_contralateral_base_1.jpg'\n",
    "    \"\"\"\n",
    "    stem, _, ext = name.rpartition(\".\")\n",
    "    ext = ext.lower() if ext else \"jpg\"\n",
    "    s = stem.strip()\n",
    "    s = re.sub(r\"\\s*\\((\\d+)\\)\\s*$\", r\"_\\1\", s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s)\n",
    "    s = re.sub(r\"([A-Za-z]+)_0*(\\d{1,3})\", lambda m: f\"{m.group(1)}_{int(m.group(2)):03d}\", s)\n",
    "\n",
    "    return s + \".\" + ext\n",
    "\n",
    "\n",
    "changed = []\n",
    "for p in root.glob(\"*.*\"):\n",
    "    if p.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        continue\n",
    "    new_name = normalize_filename(p.name)\n",
    "    if p.name != new_name:\n",
    "        new_path = p.with_name(new_name)\n",
    "        if new_path.exists():\n",
    "            print(f\"⚠️ 目标已存在，跳过: {new_path.name}\")\n",
    "            continue\n",
    "        p.rename(new_path)\n",
    "        changed.append((p.name, new_name))\n",
    "\n",
    "print(f\"✅ 已重命名 {len(changed)} 个文件:\")\n",
    "for old, new in changed[:10]:\n",
    "    print(f\" - {old}  →  {new}\")\n",
    "if len(changed) > 10:\n",
    "    print(f\" ... 共 {len(changed)} 个文件已更新。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b26441c-c268-42c8-899a-79e5700e2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已修正 831 个文件名:\n",
      " - Abbey_001_v3_contralateral_003_1.jpg  →  Abbey_001_v3_contralateral_3_1.jpg\n",
      " - Abbey_001_v3_contralateral_003_4.jpg  →  Abbey_001_v3_contralateral_3_4.jpg\n",
      " - Abbey_001_v3_contralateral_003_5.jpg  →  Abbey_001_v3_contralateral_3_5.jpg\n",
      " - Abbey_001_v3_treat_003_11.jpg  →  Abbey_001_v3_treat_3_11.jpg\n",
      " - Abbey_001_v3_treat_003_12.jpg  →  Abbey_001_v3_treat_3_12.jpg\n",
      " - Abbey_001_v3_treat_003_7.jpg  →  Abbey_001_v3_treat_3_7.jpg\n",
      " - Abbey_001_v3_treat_003_8.jpg  →  Abbey_001_v3_treat_3_8.jpg\n",
      " - Abbey_001_v4_contralateral_004_1.jpg  →  Abbey_001_v4_contralateral_4_1.jpg\n",
      " - Abbey_001_v4_contralateral_004_2.jpg  →  Abbey_001_v4_contralateral_4_2.jpg\n",
      " - Abbey_001_v4_contralateral_004_4.jpg  →  Abbey_001_v4_contralateral_4_4.jpg\n",
      " ... 共 831 个文件已更新。\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# === 设置图像文件夹路径 ===\n",
    "root = Path(r\"C:\\Users\\Charlotte\\Desktop\\dissertation\\US_new\\High_quality_images\")\n",
    "\n",
    "# === 规则：将所有中间或末尾 \"_00X\" → \"_X\"，但保留病人编号 \"Abbey_001\" 不变 ===\n",
    "def fix_all_numbers(name: str) -> str:\n",
    "    \"\"\"\n",
    "    移除除病人编号外的多余前导零\n",
    "    e.g. Abbey_001_v4_treat_004_7 -> Abbey_001_v4_treat_4_7\n",
    "    \"\"\"\n",
    "    stem, _, ext = name.rpartition(\".\")\n",
    "    ext = ext.lower()\n",
    "\n",
    "    # 保留病人编号 (第一个 \"_001\")\n",
    "    parts = stem.split(\"_\")\n",
    "    if len(parts) > 2:\n",
    "        fixed_parts = [parts[0], parts[1]]  # 前两段: Abbey, 001\n",
    "        for p in parts[2:]:\n",
    "            fixed_parts.append(re.sub(r\"^0+(\\d+)$\", r\"\\1\", p))\n",
    "        new_stem = \"_\".join(fixed_parts)\n",
    "    else:\n",
    "        new_stem = re.sub(r\"_0+(\\d+)\", r\"_\\1\", stem)\n",
    "\n",
    "    return new_stem + \".\" + ext\n",
    "\n",
    "\n",
    "# === 执行批量修改 ===\n",
    "changed = []\n",
    "for p in root.glob(\"*.*\"):\n",
    "    if p.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        continue\n",
    "    new_name = fix_all_numbers(p.name)\n",
    "    if new_name != p.name:\n",
    "        new_path = p.with_name(new_name)\n",
    "        if new_path.exists():\n",
    "            print(f\"⚠️ 目标已存在，跳过: {new_path.name}\")\n",
    "            continue\n",
    "        p.rename(new_path)\n",
    "        changed.append((p.name, new_name))\n",
    "\n",
    "print(f\"✅ 已修正 {len(changed)} 个文件名:\")\n",
    "for old, new in changed[:10]:\n",
    "    print(f\" - {old}  →  {new}\")\n",
    "if len(changed) > 10:\n",
    "    print(f\" ... 共 {len(changed)} 个文件已更新。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfa573-f32e-4311-867a-bf7fbe4a58c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
